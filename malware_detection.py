#!/usr/bin/python
# -*- coding: utf-8 -*-

import sys
import codecs
sys.stdout = codecs.getwriter('utf8')(sys.stdout)

import h2o
import math
from h2o.estimators.word2vec import H2OWord2vecEstimator
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.grid.grid_search import H2OGridSearch

#h2o.init(nthreads = -1, ip="185.28.1.254", port=54321, strict_version_check = False)
#h2o.init(ip="192.168.0.2", port=54321)
h2o.init(nthreads = -1, strict_version_check = False)

dataset_path = "G:/corpus_with_label.txt"

malware_dataset = h2o.upload_file(path=dataset_path, destination_frame = "malwaredataset", 
				col_names = ["kategori", "asm_codes"], col_types = ["enum", "string"], header = 1)

STOP_WORDS = []
				
#STOP_WORDS = ["a","acaba","alti","altmis","ama","ancak","arada","artik","asla","aslinda","aslinda","ayrica","az","bana","bazen","bazi","bazilari","belki","ben","benden","beni","benim","beri","bes","bile","bilhassa","bin","bir","biraz","bircogu","bircok","biri","birisi","birkac","birsey","biz","bizden","bize","bizi","bizim","boyle","boylece","bu","buna","bunda","bundan","bunlar","bunlari","bunlarin","bunu","bunun","burada","butun","cogu","cogunu","cok","cunku","da","daha","dahi","dan","de","defa","degil","diger","digeri","digerleri","diye","doksan","dokuz","dolayi","dolayisiyla","dort","e","edecek","eden","ederek","edilecek","ediliyor","edilmesi","ediyor","eger","elbette","elli","en","etmesi","etti","ettigi","ettigini","fakat","falan","filan","gene","geregi","gerek","gibi","gore","hala","halde","halen","hangi","hangisi","hani","hatta","hem","henuz","hep","hepsi","her","herhangi","herkes","herkese","herkesi","herkesin","hic","hicbir","hicbiri","i","i","icin","icinde","iki","ile","ilgili","ise","iste","itibaren","itibariyle","kac","kadar","karsin","kendi","kendilerine","kendine","kendini","kendisi","kendisine","kendisini","kez","ki","kim","kime","kimi","kimin","kimisi","kimse","kirk","madem","mi","mi","milyar","milyon","mu","mu","nasil","ne","neden","nedenle","nerde","nerede","nereye","neyse","nicin","nin","nin","niye","nun","nun","o","obur","olan","olarak","oldu","oldugu","oldugunu","olduklarini","olmadi","olmadigi","olmak","olmasi","olmayan","olmaz","olsa","olsun","olup","olur","olur","olursa","oluyor","on","on","ona","once","ondan","onlar","onlara","onlardan","onlari","onlarin","onu","onun","orada","ote","oturu","otuz","oyle","oysa","pek","ragmen","sana","sanki","sanki","sayet","sekilde","sekiz","seksen","sen","senden","seni","senin","sey","seyden","seye","seyi","seyler","simdi","siz","siz","sizden","sizden","size","sizi","sizi","sizin","sizin","sonra","soyle","su","suna","sunlari","sunu","ta","tabii","tam","tamam","tamamen","tarafindan","trilyon","tum","tumu","u","u","uc","un","un","uzere","var","vardi","ve","veya","ya","yahut","yani","yapacak","yapilan","yapilmasi","yapiyor","yapmak","yapti","yaptigi","yaptigini","yaptiklari","ye","yedi","yerine","yetmis","yi","yi","yine","yirmi","yoksa","yu","yuz","zaten","zira","nde"]


def tokenize(sentences, stop_word = STOP_WORDS):
    tokenized = sentences.tokenize("\\W+")
    #tokenized_lower = tokenized.tolower()
    #tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]
    #tokenized_words = tokenized_filtered[tokenized_filtered.grep("[0-9]",invert=True,output_logical=True),:]
    #tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]
    return tokenized

def predict(title, w2v, gbm):
    words = tokenize(h2o.H2OFrame(title).ascharacter())
    title_vec = w2v.transform(words, aggregate_method="AVERAGE")
    print(gbm.predict(test_data=title_vec))

print("Break malware asm codes into sequence of words")
words = tokenize(malware_dataset["asm_codes"])


print("Build word2vec model")

w2v_model = H2OWord2vecEstimator(model_id = "word2vec_model_malware" , init_learning_rate=1.0, window_size=5, vec_size=200, sent_sample_rate = 0.0, epochs = 10)

w2v_model.train(training_frame=words)

print("Calculate a vector for each malware asm codes")
malware_vecs = w2v_model.transform(words, aggregate_method = "AVERAGE")

print("Prepare training&validation data (keep only malware made of known words)")
valid_malware_codes = ~ malware_vecs["C1"].isna()
data = malware_dataset[valid_malware_codes,:].cbind(malware_vecs[valid_malware_codes,:])
train_split, valid_split = data.split_frame(ratios = [0.8])


#Build initial GBM Model
gbm_grid = H2OGradientBoostingEstimator(
        ## more trees is better if the learning rate is small enough 
        ## here, use "more than enough" trees - we have early stopping
        ntrees=1000,
        ## smaller learning rate is better
        ## since we have learning_rate_annealing, we can afford to start with a 
        #bigger learning rate
        learn_rate=0.05,
        ## learning rate annealing: learning_rate shrinks by 1% after every tree 
        ## (use 1.00 to disable, but then lower the learning_rate)
        learn_rate_annealing = 0.99,
        ## sample 80% of rows per tree
        sample_rate = 0.8,
        ## sample 80% of columns per split
        col_sample_rate = 0.8,
        ## fix a random number generator seed for reproducibility
        seed = 1234,
        ## score every 10 trees to make early stopping reproducible 
        #(it depends on the scoring interval)
        score_tree_interval = 10, 
        ## early stopping once the validation AUC doesn't improve by at least 0.01% for 
        #5 consecutive scoring events
        stopping_rounds = 5,
        stopping_metric = "AUTO",
        stopping_tolerance = 1e-4)



## Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = {'max_depth' : range(1,30,2)}

#Build grid search with previously made GBM and hyper parameters
grid = H2OGridSearch(gbm_grid, hyper_params,
                         grid_id = 'depth_grid',
                         search_criteria = {'strategy': "Cartesian"})
						 
#Train grid search
grid.train(x=malware_vecs.columns,
           y="kategori",
           training_frame = train_split,
           validation_frame = valid_split)
						 
						 

## sort the grid models by decreasing AUC
sorted_grid = grid.get_grid(decreasing=True)

max_depths = sorted_grid.sorted_metric_table()['max_depth'][0:5]
new_max = int(max(max_depths, key=int))
new_min = int(min(max_depths, key=int))

print "MaxDepth", new_max
print "MinDepth", new_min

# create hyperameter and search criteria lists (ranges are inclusive..exclusive))
hyper_params_tune = {'max_depth' : list(range(new_min,new_max+1,1)),
                'sample_rate': [x/100. for x in range(20,101)],
                'col_sample_rate' : [x/100. for x in range(20,101)],
                'col_sample_rate_per_tree': [x/100. for x in range(20,101)],
                'col_sample_rate_change_per_level': [x/100. for x in range(90,111)],
                'min_rows': [2**x for x in range(0,int(math.log(train_split.nrow,2)-1)+1)],
                'nbins': [2**x for x in range(4,11)],
                'nbins_cats': [2**x for x in range(4,13)],
                'min_split_improvement': [0,1e-8,1e-6,1e-4],
                'histogram_type': ["UniformAdaptive","QuantilesGlobal","RoundRobin"]}
search_criteria_tune = {'strategy': "RandomDiscrete",
                   'max_runtime_secs': 3600,  ## limit the runtime to 60 minutes
                   'max_models': 100,  ## build no more than 100 models
                   'seed' : 1234,
                   'stopping_rounds' : 5,
                   'stopping_metric' : "AUTO",
                   'stopping_tolerance': 1e-3
                   }

gbm_final_grid = H2OGradientBoostingEstimator(
                    ## more trees is better if the learning rate is small enough 
                    ## here, use "more than enough" trees - we have early stopping
                    ntrees=1000,
                    ## smaller learning rate is better
                    ## since we have learning_rate_annealing, we can afford to start with a 
                    #bigger learning rate
                    learn_rate=0.05,
                    ## learning rate annealing: learning_rate shrinks by 1% after every tree 
                    ## (use 1.00 to disable, but then lower the learning_rate)
                    learn_rate_annealing = 0.99,
                    ## score every 10 trees to make early stopping reproducible 
                    #(it depends on the scoring interval)
                    score_tree_interval = 10,
                    ## fix a random number generator seed for reproducibility
                    seed = 1234,
                    ## early stopping once the validation AUC doesn't improve by at least 0.01% for 
                    #5 consecutive scoring events
                    stopping_rounds = 5,
                    stopping_metric = "AUTO",
                    stopping_tolerance = 1e-4)
            
#Build grid search with previously made GBM and hyper parameters
final_grid = H2OGridSearch(gbm_final_grid, hyper_params = hyper_params_tune,
                                    grid_id = 'final_grid',
                                    search_criteria = search_criteria_tune)
#Train grid search
final_grid.train(x=malware_vecs.columns,
           y="kategori",
           ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
           max_runtime_secs = 3600, 
           training_frame = train_split,
           validation_frame = valid_split)

#print final_grid
				   
## Sort the grid models by AUC
sorted_final_grid = final_grid.get_grid(decreasing=True)

print sorted_final_grid

best_model = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][1])
best_model.model_id = "malware_kategori_tespit"

preds = best_model.predict(data)
print preds